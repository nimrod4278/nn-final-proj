report-denser(4 mid layers) with an early stop

Average SSIM score:  0.6319003568910494
epoches = 3
lr = 1e-4 # learning rate
weight_decay = 1e-2

model
## HELPER nested model
class ConvBlock(nn.Module):
    """
    A Convolutional Block that consists of two convolution layers each followed by
    instance normalization, relu activation and dropout.
    """

    def __init__(self, in_chans, out_chans, stride=1):
        """
        Args:
            in_chans (int): Number of channels in the input
            out_chans (int): Number of channels in the output 
        """
        super().__init__()

        self.in_chans = in_chans
        self.out_chans = out_chans
        self.stride = stride

        self.layers = nn.Sequential(
            nn.Conv2d(in_chans, out_chans, kernel_size=5, padding=2, stride=stride, bias=True),
            nn.InstanceNorm2d(out_chans),
            nn.LeakyReLU(),

            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, stride=1, bias=True),
            nn.InstanceNorm2d(out_chans),
            nn.LeakyReLU()
        )

    def forward(self, input):
        return self.layers(input)



class MRIModel(nn.Module):
    """
    PyTorch implementation of a U-Net mode
    This is based on:
      
    """
    def __init__(self, in_chans, out_chans, chans, num_pool_layers=4, num_depth_blocks=4):
        super().__init__()
        # test up sampling after down sampling
        self.chans = chans
        self.in_chans = in_chans
        self.out_chans = out_chans
        self.drop_prob = 0.0
        self.num_pool_layers = num_pool_layers
        self.num_depth_blocks = num_depth_blocks


        # First block should have no reduction in feature map size.
        self.phase_head = ConvBlock(in_chans=in_chans, out_chans=chans, stride=1)
        # self.amplitude_head = ConvBlock(in_chans=in_chans, out_chans=chans, stride=1)
        self.down_sample_layers = nn.ModuleList([self.phase_head])

        ch = chans
        for _ in range(num_pool_layers - 1):
            conv = ConvBlock(in_chans=ch, out_chans=ch * 2, stride=2)
            self.down_sample_layers.append(conv)
            ch *= 2

        # Size reduction happens at the beginning of a block, hence the need for stride here.
        self.mid_conv = ConvBlock(in_chans=ch, out_chans=ch, stride=2)
        self.middle_layers = nn.ModuleList()
        for _ in range(num_depth_blocks - 1):
            self.middle_layers.append(ConvBlock(in_chans=ch, out_chans=ch, stride=1))

        self.up_sample_layers = nn.ModuleList()
        for _ in range(num_pool_layers - 1):
            conv = ConvBlock(in_chans=ch * 2, out_chans=ch // 2, stride=1)
            self.up_sample_layers.append(conv)
            ch //= 2
        else:  # Last block of up-sampling.
            conv = ConvBlock(in_chans=ch * 2, out_chans=ch, stride=1)
            self.up_sample_layers.append(conv)
            assert chans == ch, 'Channel indexing error!'

        self.final_layers = nn.Sequential(
            nn.Conv2d(in_channels=ch, out_channels=ch, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=ch, out_channels=out_chans, kernel_size=1)
        )

    def forward(self, tensor, masks):
        """
        Args:
            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]
        Returns:
            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]
        """
        stack = list()
        output = tensor.unsqueeze(1)

        # Down-Sampling
        for layer in self.down_sample_layers:
            output = layer(output)
            # print("output-shape ", output.shape)
            stack.append(output)

        # Middle blocks
        # print("before output-middle ", output.shape)
        output = self.mid_conv(output)
        # print("output-middle ", output.shape)
        for layer in self.middle_layers:
            output = output + layer(output)  # Residual layers in the middle.
            # print("output-middle ", output.shape)

        # Up-Sampling.
        for layer in self.up_sample_layers:
            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)
            ds_output = stack.pop()
            # print("concat down|up: ", ds_output.shape, " * ", output.shape)
            output = torch.cat([output, ds_output], dim=1)
            output = layer(output)

#         return self.final_layers(output).squeeze(0)
        return self.final_layers(output).squeeze(0)
