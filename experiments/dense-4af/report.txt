report-dense


epoches = 3
lr = 1e-3 # learning rate
weight_decay = 0
Average SSIM score:  0.6412835419166353

model

## HELPER nested model
class ConvBlock(nn.Module):
    """
    A Convolutional Block that consists of two convolution layers each followed by
    instance normalization, relu activation and dropout.
    """

    def __init__(self, in_chans, out_chans):
        """
        Args:
            in_chans (int): Number of channels in the input.
            out_chans (int): Number of channels in the output.
            drop_prob (float): Dropout probability.
        """
        super().__init__()

        self.in_chans = in_chans
        self.out_chans = out_chans

        self.layers = nn.Sequential(
            nn.Conv2d(in_chans, out_chans, kernel_size=5, padding=2, stride=1),
            nn.InstanceNorm2d(out_chans),
            nn.ReLU(),

            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, stride=1),
            nn.InstanceNorm2d(out_chans),
            nn.ReLU()
        )

    def forward(self, input):
        return self.layers(input)



# main model:

#To calculate padding, input_size + 2 * padding_size-(filter_size-1). 
# For above case, (50+(2*1)-(3–1) = 52–2 = 50) which gives as a same input size.

class MRIModel(nn.Module):
    def __init__(self, in_chans, out_chans, chans=32, num_pool_layers=4):
        super().__init__()
        # test up sampling after down sampling
        self.chans = chans
        self.in_chans = in_chans
        self.out_chans = out_chans
        self.num_pool_layers = num_pool_layers

        ch = chans
        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, ch)]) # 1 -> 32
        # 32 -> 64 -> 128 -> 256
        for i in range(num_pool_layers - 1):
            self.down_sample_layers += [ConvBlock(ch, ch * 2)]
            ch *= 2
        
        self.conv = ConvBlock(ch, ch) # 256x256 scan

        self.up_sample_layers = nn.ModuleList()
        # (since we cat with the previous layers resulted matrices)
        # 512 -> 128, 256 -> 64, 128 -> 32, 64 -> 32
        for i in range(num_pool_layers - 1):
            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2)]
            ch //= 2
        self.up_sample_layers += [ConvBlock(ch * 2, ch)]
        self.conv2 = nn.Sequential(
            nn.Conv2d(ch, ch // 2, kernel_size=1),
            nn.Conv2d(ch // 2, ch, kernel_size=1),
            nn.Conv2d(ch, out_chans, kernel_size=1),
        )
        

    def forward(self, inputs, masks):
        stack = []
        output = inputs.unsqueeze(0)
        # Apply down-sampling layers
        for layer in self.down_sample_layers:
            output = layer(output)
            stack.append(output)
            # print("down matrix: ", output.shape)
            output = F.max_pool2d(output, kernel_size=2)
            # print("down pool matrix: ", output.shape)

        output = self.conv(output)
        for layer in self.up_sample_layers:
            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)
            downsample_matrix = stack.pop()
            # print("reducing (up|down): ", output.shape, " * ", downsample_matrix.shape)
            output = torch.cat([output, downsample_matrix], dim=1)
            # print("resulted matrix: ", output.shape)
            output = layer(output)
        # print("last layer result", output.shape)
        return self.conv2(output).squeeze(0)

## HELPER nested model
class ConvBlock(nn.Module):
    """
    A Convolutional Block that consists of two convolution layers each followed by
    instance normalization, relu activation and dropout.
    """

    def __init__(self, in_chans, out_chans, stride=1):
        """
        Args:
            in_chans (int): Number of channels in the input
            out_chans (int): Number of channels in the output 
        """
        super().__init__()

        self.in_chans = in_chans
        self.out_chans = out_chans
        self.stride = stride

        self.layers = nn.Sequential(
            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, stride=stride, bias=True),
            nn.InstanceNorm2d(out_chans),
            nn.LeakyReLU(),

            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, stride=1, bias=True),
            nn.InstanceNorm2d(out_chans),
            nn.LeakyReLU()
        )

    def forward(self, input):
        return self.layers(input)


# main model:

class MRIModel(nn.Module):
    """
    PyTorch implementation of a U-Net mode
    This is based on:
      
    """
    def __init__(self, in_chans, out_chans, chans, num_pool_layers=4, num_depth_blocks=3):
        super().__init__()
        # test up sampling after down sampling
        self.chans = chans
        self.in_chans = in_chans
        self.out_chans = out_chans
        self.drop_prob = 0.0
        self.num_pool_layers = num_pool_layers
        self.num_depth_blocks = num_depth_blocks


        # First block should have no reduction in feature map size.
        self.phase_head = ConvBlock(in_chans=in_chans, out_chans=chans, stride=1)
        # self.amplitude_head = ConvBlock(in_chans=in_chans, out_chans=chans, stride=1)
        self.down_sample_layers = nn.ModuleList([self.phase_head])

        ch = chans
        for _ in range(num_pool_layers - 1):
            conv = ConvBlock(in_chans=ch, out_chans=ch * 2, stride=2)
            self.down_sample_layers.append(conv)
            ch *= 2

        # Size reduction happens at the beginning of a block, hence the need for stride here.
        self.mid_conv = ConvBlock(in_chans=ch, out_chans=ch, stride=2)
        self.middle_layers = nn.ModuleList()
        for _ in range(num_depth_blocks - 1):
            self.middle_layers.append(ConvBlock(in_chans=ch, out_chans=ch, stride=1))

        self.up_sample_layers = nn.ModuleList()
        for _ in range(num_pool_layers - 1):
            conv = ConvBlock(in_chans=ch * 2, out_chans=ch // 2, stride=1)
            self.up_sample_layers.append(conv)
            ch //= 2
        else:  # Last block of up-sampling.
            conv = ConvBlock(in_chans=ch * 2, out_chans=ch, stride=1)
            self.up_sample_layers.append(conv)
            assert chans == ch, 'Channel indexing error!'

        self.final_layers = nn.Sequential(
            nn.Conv2d(in_channels=ch, out_channels=ch, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=ch, out_channels=out_chans, kernel_size=1)
        )

    def forward(self, tensor, masks):
        """
        Args:
            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]
        Returns:
            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]
        """
        stack = list()
        output = tensor.unsqueeze(1)

        # Down-Sampling
        for layer in self.down_sample_layers:
            output = layer(output)
            # print("output-shape ", output.shape)
            stack.append(output)

        # Middle blocks
        # print("before output-middle ", output.shape)
        output = self.mid_conv(output)
        # print("output-middle ", output.shape)
        for layer in self.middle_layers:
            output = output + layer(output)  # Residual layers in the middle.
            # print("output-middle ", output.shape)

        # Up-Sampling.
        for layer in self.up_sample_layers:
            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)
            ds_output = stack.pop()
            # print("concat down|up: ", ds_output.shape, " * ", output.shape)
            output = torch.cat([output, ds_output], dim=1)
            output = layer(output)

        return self.final_layers(output).squeeze(0)
        # return (tensor + output) if self.use_residual else output
