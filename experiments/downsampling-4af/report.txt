report-downsampling

epoches = 3
lr = 1e-3 # learning rate
weight_decay = 0
Average SSIM score:  0.6420249615900582

model
## HELPER nested model
class ConvBlock(nn.Module):
    """
    A Convolutional Block that consists of two convolution layers each followed by
    instance normalization, relu activation and dropout.
    """

    def __init__(self, in_chans, out_chans):
        """
        Args:
            in_chans (int): Number of channels in the input.
            out_chans (int): Number of channels in the output.
            drop_prob (float): Dropout probability.
        """
        super().__init__()

        self.in_chans = in_chans
        self.out_chans = out_chans

        self.layers = nn.Sequential(
            nn.Conv2d(in_chans, out_chans, kernel_size=5, padding=2, stride=1),
            nn.InstanceNorm2d(out_chans),
            nn.ReLU(),

            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, stride=1),
            nn.InstanceNorm2d(out_chans),
            nn.ReLU()
        )

    def forward(self, input):
        return self.layers(input)



# main model:

#To calculate padding, input_size + 2 * padding_size-(filter_size-1). 
# For above case, (50+(2*1)-(3–1) = 52–2 = 50) which gives as a same input size.

class MRIModel(nn.Module):
    def __init__(self, in_chans, out_chans, chans=32, num_pool_layers=4):
        super().__init__()
        # test up sampling after down sampling
        self.chans = chans
        self.in_chans = in_chans
        self.out_chans = out_chans
        self.num_pool_layers = num_pool_layers

        ch = chans
        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, ch)]) # 1 -> 32
        # 32 -> 64 -> 128 -> 256
        for i in range(num_pool_layers - 1):
            self.down_sample_layers += [ConvBlock(ch, ch * 2)]
            ch *= 2
        
        self.conv = ConvBlock(ch, ch) # 256x256 scan

        self.up_sample_layers = nn.ModuleList()
        # (since we cat with the previous layers resulted matrices)
        # 512 -> 128, 256 -> 64, 128 -> 32, 64 -> 32
        for i in range(num_pool_layers - 1):
            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2)]
            ch //= 2
        self.up_sample_layers += [ConvBlock(ch * 2, ch)]
        self.conv2 = nn.Sequential(
            nn.Conv2d(ch, ch // 2, kernel_size=1),
            nn.Conv2d(ch // 2, ch, kernel_size=1),
            nn.Conv2d(ch, out_chans, kernel_size=1),
        )
        

    def forward(self, inputs, masks):
        stack = []
        output = inputs.unsqueeze(0)
        # Apply down-sampling layers
        for layer in self.down_sample_layers:
            output = layer(output)
            stack.append(output)
            # print("down matrix: ", output.shape)
            output = F.max_pool2d(output, kernel_size=2)
            # print("down pool matrix: ", output.shape)

        output = self.conv(output)
        for layer in self.up_sample_layers:
            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)
            downsample_matrix = stack.pop()
            # print("reducing (up|down): ", output.shape, " * ", downsample_matrix.shape)
            output = torch.cat([output, downsample_matrix], dim=1)
            # print("resulted matrix: ", output.shape)
            output = layer(output)
        # print("last layer result", output.shape)
        return self.conv2(output).squeeze(0)


