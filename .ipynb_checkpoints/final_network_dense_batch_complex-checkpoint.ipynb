{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI reconstruction\n",
    "In this notebook we'll show our implementation of the neural network that reconstruct images from undersampled h5 files in order to provide a faster MRI scan processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import h5py, os\n",
    "import pytorch_ssim\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # check whether a GPU is available\n",
    "from skimage.measure import compare_ssim\n",
    "import sys\n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader methods\n",
    "provided methods to imports the data.\n",
    "\n",
    "And also the batch_size const we'll use for retrieving the data loader batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "current_mask = 8\n",
    "PATH = \"une\" # path for the model saved paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subject_id = self.data_list[index]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id\n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    m = nn.ZeroPad2d(((512-Nx)//2, (512-Nx) // 2, 0, 0))\n",
    "    # we're adding padding on the width to have it consisted in our data set\n",
    "    slice_kspace = slice_kspace.permute(0,3,1,2)\n",
    "    slice_kspace = m(slice_kspace)\n",
    "    slice_kspace = slice_kspace.permute(0,2,3,1)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "    img_gt = T.complex_center_crop(img_gt.squeeze(0), [320,320])\n",
    "    img_und = T.complex_center_crop(img_und.squeeze(0), [320,320])\n",
    "        \n",
    "    return img_gt, img_und, rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)): # 0: train_path , 1: val_path\n",
    "        print(\"dataset-loader: opening ... \", data_path[i])\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path, fname) # fetch one h5 file from the path\n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                if 'kspace' in data:\n",
    "                    num_slice = data['kspace'].shape[0]\n",
    "                else:\n",
    "                    num_slice = data['kspace_4af'].shape[0]  if current_mask == 4 else data['kspace_8af'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Initialize the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset-loader: opening ...  /data/local/NC2019MRI/train\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/data/local/NC2019MRI/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d6faea08e450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_path_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/data/local/NC2019MRI/train'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_path_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/data/local/NC2019MRI/test'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmask4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'acc'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cen_fract'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.08\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8615df21a910>\u001b[0m in \u001b[0;36mload_data_path\u001b[1;34m(train_data_path, val_data_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mwhich_data_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhich_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0msubject_data_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhich_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fetch one h5 file from the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/data/local/NC2019MRI/train'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path_train = '/data/local/NC2019MRI/train'\n",
    "data_path_val = '/data/local/NC2019MRI/test'\n",
    "data_list = load_data_path(data_path_train, data_path_val)\n",
    "\n",
    "mask4 = { 'acc': 4, 'cen_fract': 0.08 }\n",
    "mask8 = { 'acc': 8, 'cen_fract': 0.04 }\n",
    "\n",
    "mask = mask4 if current_mask == 4 else mask8\n",
    "acc = mask['acc']\n",
    "cen_fract = mask['cen_fract']\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "\n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "len_dataset = len(dataset)\n",
    "indx = np.arange(len_dataset)\n",
    "train_indx = indx[:int(len_dataset*0.8)]\n",
    "\n",
    "val_indx = indx[-(int(len_dataset*0.2)):]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indx)\n",
    "val_dataset = Subset(dataset, val_indx)\n",
    "#train_dataset, val_dataset = random_split(dataset, (int(len(dataset)*0.8)+1, int(len(dataset)*0.2)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers) \n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model\n",
    "\n",
    "Here we'll construct our model:\n",
    "\n",
    "#TODO explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER nested model\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input\n",
    "            out_chans (int): Number of channels in the output \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.stride = stride\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=5, padding=2, stride=stride, bias=True),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MRIModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net mode with dense deep middle layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers=4, num_depth_blocks=3):\n",
    "        super().__init__()\n",
    "        # test up sampling after down sampling\n",
    "        self.chans = chans\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.num_depth_blocks = num_depth_blocks\n",
    "\n",
    "\n",
    "        # First block should have no reduction in feature map size.\n",
    "        # turns the inputs (2 since complex) to 32\n",
    "        self.phase_head = ConvBlock(in_chans=in_chans, out_chans=chans, stride=1)\n",
    "        self.down_sample_layers = nn.ModuleList([self.phase_head])\n",
    "\n",
    "        ch = chans\n",
    "        \"\"\"\n",
    "        First we're down sample the image while increasing the number of channels.\n",
    "        Meaning smaller parts of the image across more neurons.\n",
    "        Thus, extracting the important features of the image\n",
    "        \"\"\"\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            conv = ConvBlock(in_chans=ch, out_chans=ch * 2, stride=2)\n",
    "            self.down_sample_layers.append(conv)\n",
    "            ch *= 2\n",
    "\n",
    "        # Size reduction happens at the beginning of a block, hence the need for stride here.\n",
    "        self.mid_conv = ConvBlock(in_chans=ch, out_chans=ch, stride=2)\n",
    "        self.middle_layers = nn.ModuleList()\n",
    "        \"\"\"\n",
    "        Then we're passing the data through deep middle layers of convolutional2D.\n",
    "        Adding more paramters to the network\n",
    "        \"\"\"\n",
    "        for _ in range(num_depth_blocks - 1):\n",
    "            self.middle_layers.append(ConvBlock(in_chans=ch, out_chans=ch, stride=1))\n",
    "\n",
    "        \"\"\"\n",
    "        Lastly we're upsampled the image while concatinating it with previously features extracted\n",
    "        by the down sampler. then passing each through layers of convolutional scan.\n",
    "        Essentially emphasizing the features picked up by the down sampled version of the image.\n",
    "        \"\"\"\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            conv = ConvBlock(in_chans=ch * 2, out_chans=ch // 2, stride=1)\n",
    "            self.up_sample_layers.append(conv)\n",
    "            ch //= 2\n",
    "        else:  # Last block of up-sampling.\n",
    "            conv = ConvBlock(in_chans=ch * 2, out_chans=ch, stride=1)\n",
    "            self.up_sample_layers.append(conv)\n",
    "            assert chans == ch, 'Channel indexing error!'\n",
    "\n",
    "\n",
    "        # passing the resulted image through finalization process with 3 convolutional layers\n",
    "        # This is to try smooth the image a bit.\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ch, out_channels=ch, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=ch, out_channels=out_chans, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = list()\n",
    "        output = tensor\n",
    "\n",
    "        # Down-Sampling\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "\n",
    "        # Middle blocks\n",
    "        output = self.mid_conv(output)\n",
    "        for layer in self.middle_layers:\n",
    "            output = output + layer(output)  # Residual layers in the middle.\n",
    "        # Up-Sampling.\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            ds_output = stack.pop()\n",
    "            # concatinating with the down sample input of the same size.\n",
    "            output = torch.cat([output, ds_output], dim=1)\n",
    "            output = layer(output)\n",
    "\n",
    "        final_output = self.final_layers(output)\n",
    "        return final_output\n",
    "        # return (tensor + output) if self.use_residual else output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General purpose function to construct a train step function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_step_call(model, loss_fn, optimiser):\n",
    "\n",
    "    # define a function inside another function\n",
    "    \"\"\"\n",
    "        img_inputs = complex valued undersampled image\n",
    "        img_target = complex valued ground truth image\n",
    "    \"\"\"\n",
    "    def train_step(img_inputs, img_target): \n",
    "        # img_target = T.complex_abs(img_target)\n",
    "        \"\"\"\n",
    "         permutate the image (1, w, h, 2) -> (1, 2, w, h)\n",
    "         considering the complex numbers as two channel input\n",
    "        \"\"\"\n",
    "        inputs_perm = img_und.permute(0, 3, 1, 2)\n",
    "\n",
    "        ### foreword ###\n",
    "        output_raw_pred = model(inputs_perm) # feed forward the inputs (complex image)\n",
    "        \n",
    "        img_pred_complex = output_raw_pred.permute(0, 2, 3, 1) # permutate the image back to its origianl shape\n",
    "        \n",
    "        ### backward ###\n",
    "        optimiser.zero_grad()\n",
    "        # compute the loss using SSIM score between prediction and ground truth\n",
    "        \n",
    "        loss = loss_fn(img_target, img_pred_complex) \n",
    "        loss.backward()                   # autograd = provide gradient to update the params\n",
    "        optimiser.step()                  # update parameters\n",
    "        return loss.item()                # return the loss\n",
    "\n",
    "    # return the newly defined function\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "__Details of training:__\n",
    "* we use here 10 epoches (/iterations) to adjust the paramters.\n",
    "* For the loss function we choose to use SSIM\n",
    "* Adam method for the backpropogration optimization step. (weight_decay=)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Using generate_train_step_call to create generic step caller using model, criterion and optimizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init essential constants\n",
    "epoches = 2\n",
    "lr = 1e-4 # learning rate\n",
    "weight_decay = 0\n",
    "\n",
    "# create the main components to generate a train step\n",
    "model = MRIModel(\n",
    "    in_chans=2,\n",
    "    out_chans=2,\n",
    "    chans=32,\n",
    "    num_pool_layers=4\n",
    ").to(device)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "#loss_fn = pytorch_ssim.SSIM(window_size=11)\n",
    "# optimiser = torch.optim.RMSprop(model.parameters(), lr, weight_decay=weight_decay)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train_step = generate_train_step_call(model, criterion, optimiser)\n",
    "\n",
    "# init data loaders\n",
    "train_loader, val_loader = build_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate the epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training started\")\n",
    "losses = list()\n",
    "val_losses = list()\n",
    "running_loss = 0\n",
    "running_valid_loss = 0\n",
    "min_valid_loss = 1000\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "        \n",
    "        # send to GPU\n",
    "        img_und = img_und.to(device)\n",
    "        img_gt = img_gt.to(device)\n",
    "        \n",
    "        loss_value = train_step(img_und, img_gt)\n",
    "        # accumelators\n",
    "        running_loss += loss_value\n",
    "#         if (-loss_value) >= 0.81: break\n",
    "    \n",
    "    # performing evalutation every epoch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for iteration, sample in enumerate(val_loader):\n",
    "            img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "\n",
    "            # send to GPU\n",
    "            image_und = img_und.to(device)\n",
    "            img_gt = img_gt.to(device)\n",
    "            \n",
    "            inputs = img_und.permute(0, 3, 1, 2) # passing the same way like in training\n",
    "\n",
    "            # feed forward the inputs\n",
    "            output_raw_pred = model(inputs.to(device))\n",
    "            output_raw_pred = output_raw_pred.permute(0, 2, 3, 1)\n",
    "            val_loss_value = criterion(img_gt, output_raw_pred)\n",
    "\n",
    "            running_valid_loss += val_loss_value\n",
    "            \n",
    "    # Log the epoch loss value\n",
    "    avg_loss = running_loss/len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    avg_valid_loss = running_valid_loss/len(val_loader)\n",
    "    \n",
    "    val_losses.append(avg_valid_loss)\n",
    "    print('epoch [{}/{}], loss:{:.4f}, val_loss:{:.4f}'.format(epoch+1, epoches, avg_loss, avg_valid_loss))\n",
    "    running_loss = 0\n",
    "    running_valid_loss = 0\n",
    "    if avg_valid_loss < min_valid_loss:\n",
    "        min_valid_loss = avg_valid_loss\n",
    "        torch.save(model.state_dict(), PATH + \".pth\")\n",
    "    print(\"Best valid loss\")\n",
    "    print(\"------------\")\n",
    "    # implement early stop\n",
    "#     else:\n",
    "        \n",
    "#         continue\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the loss over time\n",
    "We can see here the loss value on each slices batch over the total epoches\n",
    "And the value represented as inverted loss of SSIM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(range(1,epoches+1), losses, label=\"Training loss\")\n",
    "plt.plot(range(1,epoches+1), val_losses, label=\"Validation loss\")\n",
    "plt.xlabel = \"Epoches\"\n",
    "plt.ylabel = \"Loss\"\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('loss' + PATH + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('unet.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/local/NC2019MRI/test/file1000817.h5'\n",
    "\n",
    "with h5py.File(file_path,  \"r\") as hf:\n",
    "    volume_kspace_4af = hf['kspace_4af'][()]\n",
    "    volume_kspace_8af = hf['kspace_8af'][()]\n",
    "    mask_4af = hf['mask_4af'][()]\n",
    "    mask_8af = hf['mask_8af'][()]\n",
    "    print(volume_kspace_4af.shape)\n",
    "    print(volume_kspace_4af.dtype)\n",
    "    print(mask_4af.shape)\n",
    "    print(mask_4af.dtype)\n",
    "\n",
    "volume_kspace2 = T.to_tensor(volume_kspace_8af)      # Convert from numpy array to pytorch tensor\n",
    "volume_image = T.ifft2(volume_kspace2)            # Apply Inverse Fourier Transform to get the complex image\n",
    "cropped_volume_image_8abs = T.complex_center_crop(volume_image.squeeze(0), [320,320])\n",
    "print(cropped_volume_image_8abs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_test = DataLoader(val_dataset, shuffle=True, batch_size=1, num_workers=num_workers)\n",
    "sample = next(iter(val_loader_test))\n",
    "\n",
    "\n",
    "img_gt, img_und, _, masks, _ = sample\n",
    "img_gt = T.complex_abs(img_gt.to(device))\n",
    "img_undersampled = T.complex_abs(img_und.to(device))\n",
    "\n",
    "inputs = img_und.to(device).permute(0, 3, 1, 2) # passing the same way like in training\n",
    "\n",
    "# feed forward the inputs\n",
    "output_raw_pred = model(inputs.to(device))\n",
    "img_pred = output_raw_pred.permute(0, 2, 3, 1)\n",
    "img_pred = T.complex_abs(img_pred)\n",
    "\n",
    "allimgs = torch.stack([img_undersampled.squeeze(0).cpu().detach(),\n",
    "                       img_pred.squeeze(0).cpu().detach(),\n",
    "                       img_gt.squeeze(0).cpu().detach()\n",
    "                      ], dim=0)\n",
    "show_slices(allimgs, [0,1,2],  cmap='gray')\n",
    "with torch.no_grad():\n",
    "    ssimloss1 = ssim(img_undersampled.cpu().detach().numpy(), \n",
    "                    img_gt.cpu().detach().numpy())\n",
    "    print(\"Random sample SSIM score (undersampled):\", ssimloss1)\n",
    "    ssimloss2 = ssim(img_pred.cpu().detach().numpy(), \n",
    "                    img_gt.cpu().detach().numpy())\n",
    "    print(\"Random sample SSIM score (predicted):\", ssimloss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = next(iter(test_loader))\n",
    "\n",
    "\n",
    "img_gt, img_und, _, masks, _ = sample\n",
    "img_gt = T.complex_abs(img_gt.to(device))\n",
    "img_undersampled = T.complex_abs(img_und.to(device))\n",
    "\n",
    "#inputs = img_und.to(device).permute(0, 3, 1, 2) # passing the same way like in training\n",
    "inputs = cropped_volume_image_8abs[-1].unsqueeze(0).permute(0, 3, 1, 2)\n",
    "img_undersampled = T.complex_abs(inputs.permute(0, 2, 3, 1))\n",
    "print(inputs.shape)\n",
    "print(img_undersampled.shape)\n",
    "# feed forward the inputs\n",
    "output_raw_pred = model(inputs.to(device))\n",
    "img_pred = output_raw_pred.permute(0, 2, 3, 1)\n",
    "img_pred = T.complex_abs(img_pred)\n",
    "\n",
    "allimgs = torch.stack([img_undersampled.squeeze(0).cpu().detach(),\n",
    "                       img_pred.squeeze(0).cpu().detach(),\n",
    "                       ], dim=0)\n",
    "show_slices(allimgs, [0,1],  cmap='gray')\n",
    "#with torch.no_grad():\n",
    "    #ssimloss = ssim(img_pred.squeeze(0).cpu().detach().numpy(), \n",
    "    #                img_gt.squeeze(0).cpu().detach().numpy())\n",
    "    #print(\"Random sample SSIM score:\", ssimloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(reconstructions, out_dir):\n",
    "    \"\"\"\n",
    "    Saves the reconstructions from a model into h5 files that is appropriate for submission\n",
    "    to the leaderboard.\n",
    "    Args:\n",
    "        reconstructions (dict[str, np.array]): A dictionary mapping input filenames to\n",
    "            corresponding reconstructions (of shape num_slices x height x width).\n",
    "        out_dir (pathlib.Path): Path to the output directory where the reconstructions\n",
    "            should be saved.\n",
    "    \"\"\"\n",
    "    for fname, recons in reconstructions.items():\n",
    "        subject_path = os.path.join(out_dir, fname)\n",
    "        print(subject_path)\n",
    "        with h5py.File(subject_path, 'a') as f:\n",
    "            f.create_dataset('recon_4af', data=recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(cropped_img):\n",
    "    inputs = cropped_img.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    img_undersampled = T.complex_abs(inputs.permute(0, 2, 3, 1))\n",
    "    # feed forward the inputs\n",
    "    output_raw_pred = model(inputs.to(device))\n",
    "    img_pred = output_raw_pred.permute(0, 2, 3, 1)\n",
    "    img_pred = T.complex_abs(img_pred)\n",
    "    return img_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/local/NC2019MRI/test/'\n",
    "\n",
    "for fname in sorted(os.listdir(file_path)):\n",
    "    subject_path = os.path.join(file_path, fname)\n",
    "    with h5py.File(subject_path,  \"r\") as hf:\n",
    "        print(f'file {fname} key is {list(hf.keys())}')\n",
    "\n",
    "        volume_kspace_4af = hf['kspace_4af'][()]\n",
    "        volume_kspace_8af = hf['kspace_8af'][()]\n",
    "        mask_4af = hf['mask_4af'][()]\n",
    "        mask_8af = hf['mask_8af'][()]\n",
    "        print(volume_kspace_4af.shape)\n",
    "        print(volume_kspace_4af.dtype)\n",
    "        print(mask_4af.shape)\n",
    "        print(mask_4af.dtype)\n",
    "\n",
    "        volume_kspace2 = T.to_tensor(volume_kspace_8af)      # Convert from numpy array to pytorch tensor\n",
    "        volume_image = T.ifft2(volume_kspace2)            # Apply Inverse Fourier Transform to get the complex image\n",
    "        cropped_volume_image_8abs = T.complex_center_crop(volume_image.squeeze(0), [320,320])\n",
    "        result = torch.Tensor()\n",
    "        for i in range(0, cropped_volume_image_8abs.shape[0]):\n",
    "            pred = predict(cropped_volume_image_8abs[i,:,:,:]).cpu().detach()\n",
    "            result = torch.cat((result,pred), 0)\n",
    "            \n",
    "        reconstructions = {fname: result.numpy()}\n",
    "        out_dir = '../saved/' # where you want to save your result. \n",
    "        if not (os.path.exists(out_dir)): os.makedirs(out_dir)\n",
    "        save_reconstructions(reconstructions, out_dir)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate\n",
    "ssim_vals = list()\n",
    "\n",
    "for iteration, sample in enumerate(val_loader):\n",
    "    img_gt, img_und, _, masks, _ = sample\n",
    "    img_target = crop320x_image(T.complex_abs(img_gt.to(device)))\n",
    "    inputs = crop320x_image(T.complex_abs(img_und.to(device)))\n",
    "\n",
    "    img_pred = model(inputs, masks)\n",
    "    with torch.no_grad():\n",
    "        ssim_vals.append(ssim(img_pred.squeeze(0).cpu().detach().numpy(), \n",
    "                              img_target.squeeze(0).cpu().detach().numpy()))\n",
    "print(\"Average SSIM score: \", np.average(ssim_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
