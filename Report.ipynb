{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq3dGv8sTiY0"
   },
   "source": [
    "\n",
    "<h1 style=\"text-align:center; margin-top:5em\">Neural Networks - Dr. Kfir Bar<h1>\n",
    "<h2 style=\"text-align:center;font-style: italic;\">\"Reconstruct images from magnetic resonance imaging (MRI)<h2>\n",
    "\n",
    "<p style=\"margin:3em 0 0.2em 5em\">Members:<p>\n",
    "<h3 style=\"margin-left:12em\">Nimrod Feldman (311129555)<h3>\n",
    "<h3 style=\"margin-left:12em\">Ron Maayan ()<h3>\n",
    "<h3 style=\"margin-left:12em\">Segev Efraim ()<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIafSM_STiZJ"
   },
   "source": [
    "### U-net model\n",
    "U-net is an autoencoder with skip connections in order to localize features more precisely for the upsampling path.\n",
    "There is a similar study about reconstructing MRI images from undersampled k-space data in the web. A web page mention different operations applied throughtout the model they propose. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hugULudsTiZP"
   },
   "source": [
    "# Implementation \n",
    "\n",
    "\n",
    "\n",
    "#### Model\n",
    "\n",
    "Changes has been made to the data loader get_epoch_batch in order to have consisted widths across the train dataset. \n",
    "Then, we cropped the images to 320x320 size, so that it will easier to feed to the neural network we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # check whether a GPU is available\n",
    "from skimage.measure import compare_ssim\n",
    "import sys\n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )\n",
    "batch_size = 8\n",
    "current_mask = 8\n",
    "PATH = \"unet\" # path for the model saved paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subject_id = self.data_list[index]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)\n",
    "\n",
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id\n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    m = nn.ZeroPad2d(((512-Nx)//2, (512-Nx) // 2, 0, 0))\n",
    "    # we're adding padding on the width to have it consisted in our data set\n",
    "    slice_kspace = slice_kspace.permute(0,3,1,2)\n",
    "    slice_kspace = m(slice_kspace)\n",
    "    slice_kspace = slice_kspace.permute(0,2,3,1)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "    img_gt = T.complex_center_crop(img_gt.squeeze(0), [320,320])\n",
    "    img_und = T.complex_center_crop(img_und.squeeze(0), [320,320])\n",
    "        \n",
    "    return img_gt, img_und, rawdata_und.squeeze(0), masks.squeeze(0), norm\n",
    "\n",
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)): # 0: train_path , 1: val_path\n",
    "        print(\"dataset-loader: opening ... \", data_path[i])\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path, fname) # fetch one h5 file from the path\n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                if 'kspace' in data:\n",
    "                    num_slice = data['kspace'].shape[0]\n",
    "                else:\n",
    "                    num_slice = data['kspace_4af'].shape[0]  if current_mask == 4 else data['kspace_8af'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset-loader: opening ...  ./train\n",
      "dataset-loader: opening ...  ./train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path_train = './train'\n",
    "data_path_val = './train'\n",
    "data_list = load_data_path(data_path_train, data_path_val)\n",
    "\n",
    "mask4 = { 'acc': 4, 'cen_fract': 0.08 }\n",
    "mask8 = { 'acc': 8, 'cen_fract': 0.04 }\n",
    "\n",
    "mask = mask4 if current_mask == 4 else mask8\n",
    "acc = mask['acc']\n",
    "cen_fract = mask['cen_fract']\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 0 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "\n",
    "# create data loader for training set. \n",
    "# It applies same to validation set as well\n",
    "dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "len_dataset = len(dataset)\n",
    "indx = np.arange(len_dataset)\n",
    "\n",
    "train_indx = indx[:int(len_dataset*0.8)]\n",
    "val_indx = indx[-(int(len_dataset*0.2)):]\n",
    "\n",
    "# we divide our train dataset into 80/20 split (not randomly)\n",
    "# in order to have validation dataset seperately.\n",
    "train_dataset = Subset(dataset, train_indx)\n",
    "val_dataset = Subset(dataset, val_indx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers) \n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulZJYrbqMW8r"
   },
   "outputs": [],
   "source": [
    "## HELPER nested model\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input\n",
    "            out_chans (int): Number of channels in the output \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.stride = stride\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=5, padding=2, stride=stride, bias=True),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "\n",
    "class MRIModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net mode with dense deep middle layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers=4, num_depth_blocks=3):\n",
    "        super().__init__()\n",
    "        # test up sampling after down sampling\n",
    "        self.chans = chans\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.num_depth_blocks = num_depth_blocks\n",
    "\n",
    "\n",
    "        # First block should have no reduction in feature map size.\n",
    "        # turns the inputs (2 since complex) to 32\n",
    "        self.phase_head = ConvBlock(in_chans=in_chans, out_chans=chans, stride=1)\n",
    "        self.down_sample_layers = nn.ModuleList([self.phase_head])\n",
    "\n",
    "        ch = chans\n",
    "        \"\"\"\n",
    "        First we're down sample the image while increasing the number of channels.\n",
    "        Meaning smaller parts of the image across more neurons.\n",
    "        Thus, extracting the important features of the image\n",
    "        \"\"\"\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            conv = ConvBlock(in_chans=ch, out_chans=ch * 2, stride=2)\n",
    "            self.down_sample_layers.append(conv)\n",
    "            ch *= 2\n",
    "\n",
    "        # Size reduction happens at the beginning of a block, hence the need for stride here.\n",
    "        self.mid_conv = ConvBlock(in_chans=ch, out_chans=ch, stride=2)\n",
    "        self.middle_layers = nn.ModuleList()\n",
    "        \"\"\"\n",
    "        Then we're passing the data through deep middle layers of convolutional2D.\n",
    "        Adding more paramters to the network\n",
    "        \"\"\"\n",
    "        for _ in range(num_depth_blocks - 1):\n",
    "            self.middle_layers.append(ConvBlock(in_chans=ch, out_chans=ch, stride=1))\n",
    "\n",
    "        \"\"\"\n",
    "        Lastly we're upsampled the image while concatinating it with previously features extracted\n",
    "        by the down sampler. then passing each through layers of convolutional scan.\n",
    "        Essentially emphasizing the features picked up by the down sampled version of the image.\n",
    "        \"\"\"\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            conv = ConvBlock(in_chans=ch * 2, out_chans=ch // 2, stride=1)\n",
    "            self.up_sample_layers.append(conv)\n",
    "            ch //= 2\n",
    "        else:  # Last block of up-sampling.\n",
    "            conv = ConvBlock(in_chans=ch * 2, out_chans=ch, stride=1)\n",
    "            self.up_sample_layers.append(conv)\n",
    "            assert chans == ch, 'Channel indexing error!'\n",
    "\n",
    "\n",
    "        # passing the resulted image through finalization process with 3 convolutional layers\n",
    "        # This is to try smooth the image a bit.\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ch, out_channels=ch, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=ch, out_channels=out_chans, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = list()\n",
    "        output = tensor\n",
    "\n",
    "        # Down-Sampling\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "\n",
    "        # Middle blocks\n",
    "        output = self.mid_conv(output)\n",
    "        for layer in self.middle_layers:\n",
    "            output = output + layer(output)  # Residual layers in the middle.\n",
    "        # Up-Sampling.\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            ds_output = stack.pop()\n",
    "            # concatinating with the down sample input of the same size.\n",
    "            output = torch.cat([output, ds_output], dim=1)\n",
    "            output = layer(output)\n",
    "\n",
    "        final_output = self.final_layers(output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### We used a train_step generator function preseted here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_step_call(model, loss_fn, optimiser):\n",
    "\n",
    "    # define a function inside another function\n",
    "    \"\"\"\n",
    "        img_inputs = complex valued undersampled image\n",
    "        img_target = complex valued ground truth image\n",
    "    \"\"\"\n",
    "    def train_step(img_inputs, img_target): \n",
    "        # img_target = T.complex_abs(img_target)\n",
    "        \"\"\"\n",
    "         permutate the image (1, w, h, 2) -> (1, 2, w, h)\n",
    "         considering the complex numbers as two channel input\n",
    "        \"\"\"\n",
    "        inputs_perm = img_und.permute(0, 3, 1, 2)\n",
    "\n",
    "        ### foreword ###\n",
    "        output_raw_pred = model(inputs_perm) # feed forward the inputs (complex image)\n",
    "\n",
    "        # permutate the image back to its origianl shape\n",
    "        img_pred_complex = output_raw_pred.permute(0, 2, 3, 1) \n",
    "        \n",
    "        ### backward ###\n",
    "        optimiser.zero_grad()\n",
    "        # compute the loss using SSIM score between prediction and ground truth\n",
    "        \n",
    "        loss = loss_fn(img_target, img_pred_complex) \n",
    "        loss.backward()          # autograd = provide gradient to update the params\n",
    "        optimiser.step()         # update parameters\n",
    "        return loss.item()       # return the loss\n",
    "\n",
    "    # return the newly defined function\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiFX3ORTM4-D"
   },
   "source": [
    "\n",
    "#### Optimizer\n",
    "\n",
    "The detailed optimizer code is written below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTojdZg-RQJN"
   },
   "outputs": [],
   "source": [
    "epoches = 100\n",
    "lr = 1e-3 # learning rate\n",
    "weight_decay = 0\n",
    "\n",
    "# create the main components to generate a train step\n",
    "model = MRIModel(\n",
    "    in_chans=2,\n",
    "    out_chans=2,\n",
    "    chans=32,\n",
    "    num_pool_layers=4\n",
    ").to(device)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train_step = generate_train_step_call(model, criterion, optimiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qjgYJ0FRaM4"
   },
   "source": [
    "The optimiser we decided to use is ADAM, which is the fastest one of the bunch.\n",
    "Also has better efficiency in terms memory and computional complexity. And its appropriate for problems with very noisy or sparse gradients.\n",
    "\n",
    "We've decided to feed the network the complex numbers to have 2 channels in and out and using 4 layers for the autoencoder down and up sampling parts.\n",
    "\n",
    "\n",
    "The loss function we've used is MSE. MSE is the most popular one and perform very good in most scenarios. We tried to use different loss functions such as L1, SSIM and MSE.\n",
    "\n",
    "SSIM was a good loss funciton but wasn't very computionally efficent so we used MSE as the final choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop\n",
    "\n",
    "Train loop includes an evaluation block.\n",
    "We've also implemented checkpoints on each epoch iteration saving only improved models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e65d19898abc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mimg_gt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_gt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_und\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_gt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m# accumelators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9d1f01a99ada>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(img_inputs, img_target)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_pred_complex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# autograd = provide gradient to update the params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# return the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training started\")\n",
    "losses = list()\n",
    "val_losses = list()\n",
    "running_loss = 0\n",
    "running_valid_loss = 0\n",
    "min_valid_loss = 1000\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "        \n",
    "        # send to GPU\n",
    "        img_und = img_und.to(device)\n",
    "        img_gt = img_gt.to(device)\n",
    "        \n",
    "        loss_value = train_step(img_und, img_gt)\n",
    "        # accumelators\n",
    "        running_loss += loss_value\n",
    "    \n",
    "    # performing evalutation every epoch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for iteration, sample in enumerate(val_loader):\n",
    "            img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "\n",
    "            # send to GPU\n",
    "            image_und = img_und.to(device)\n",
    "            img_gt = img_gt.to(device)\n",
    "            \n",
    "            inputs = img_und.permute(0, 3, 1, 2) # passing the same way like in training\n",
    "\n",
    "            # feed forward the inputs\n",
    "            output_raw_pred = model(inputs.to(device))\n",
    "            output_raw_pred = output_raw_pred.permute(0, 2, 3, 1)\n",
    "            val_loss_value = criterion(img_gt, output_raw_pred)\n",
    "\n",
    "            running_valid_loss += val_loss_value\n",
    "            \n",
    "    # Log the epoch loss value\n",
    "    avg_loss = running_loss/len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    avg_valid_loss = running_valid_loss/len(val_loader)\n",
    "    \n",
    "    val_losses.append(avg_valid_loss)\n",
    "    print('epoch [{}/{}], loss:{:.4f}, val_loss:{:.4f}'.format(epoch+1, epoches, avg_loss, avg_valid_loss))\n",
    "    running_loss = 0\n",
    "    running_valid_loss = 0\n",
    "    if avg_valid_loss < min_valid_loss:\n",
    "        min_valid_loss = avg_valid_loss\n",
    "        torch.save(model.state_dict(), PATH + \".pth\")\n",
    "        print(\"Best valid loss\")\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImSSJEdnTiZX"
   },
   "source": [
    "\n",
    "\n",
    "# Experiments\n",
    "\n",
    "#### Training set, validation set and test set\n",
    "We have splitted the training set in two parts: a training set for training the model and a validation set for validating the model. We validate a model focusing on the loss function, in the training process we reduce the loss function at each epoch. In the meanwhile, we need to know when to stop to avoid overfitting. To solve this problem, we  stop the training when results of the model in the validation set don't improve anymore or conversely they start to deteriorate. We will only use the test set to measure the results because the test set needs to be totally isolated of the model training and evaluation.\n",
    "#### Batch size\n",
    "To reduce overfitting, we train the model per batchs. A batch is set of samples. A batch size which is greater than 1 could allow us to update the gradient with the average results for each batch and not individually for each sample. By this method, we can avoid the model to memorise individual samples.\n",
    "#### Dense 4-fold\n",
    "In order to improve the results, we have used a variant of the u-net neural network: the dense u-net approach. In a dense u-net, we can add more layers inside the u-net process in which we apply convolutions which keep the size of the views and the number of channels. We can see an example of a dense u-net model in the figure below.\n",
    "\n",
    "Therefore, in our model, we have added the 3 convolution layers keeping the size and number of channels. We have added these extra layers in the middle of the process of the previous model. The code of our dense u-net model is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2UVsJmHj2qE"
   },
   "source": [
    "```python\n",
    "## HELPER nested model\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input\n",
    "            out_chans (int): Number of channels in the output \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.stride = stride\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=5, padding=2, stride=stride, bias=True),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "\n",
    "\n",
    "\n",
    "class MRIModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net mode\n",
    "    This is based on:\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers=4, num_depth_blocks=4):\n",
    "        super().__init__()\n",
    "        # test up sampling after down sampling\n",
    "        self.chans = chans\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = 0.0\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.num_depth_blocks = num_depth_blocks\n",
    "\n",
    "\n",
    "        # First block should have no reduction in feature map size.\n",
    "        self.phase_head = ConvBlock(in_chans=in_chans, out_chans=chans, stride=1)\n",
    "        self.down_sample_layers = nn.ModuleList([self.phase_head])\n",
    "\n",
    "        ch = chans\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            conv = ConvBlock(in_chans=ch, out_chans=ch * 2, stride=2)\n",
    "            self.down_sample_layers.append(conv)\n",
    "            ch *= 2\n",
    "\n",
    "        # Size reduction happens at the beginning of a block, hence the need for stride here.\n",
    "        self.mid_conv = ConvBlock(in_chans=ch, out_chans=ch, stride=2)\n",
    "        self.middle_layers = nn.ModuleList()\n",
    "        for _ in range(num_depth_blocks - 1):\n",
    "            self.middle_layers.append(ConvBlock(in_chans=ch, out_chans=ch, stride=1))\n",
    "\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for _ in range(num_pool_layers - 1):\n",
    "            conv = ConvBlock(in_chans=ch * 2, out_chans=ch // 2, stride=1)\n",
    "            self.up_sample_layers.append(conv)\n",
    "            ch //= 2\n",
    "        else:  # Last block of up-sampling.\n",
    "            conv = ConvBlock(in_chans=ch * 2, out_chans=ch, stride=1)\n",
    "            self.up_sample_layers.append(conv)\n",
    "            assert chans == ch, 'Channel indexing error!'\n",
    "\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ch, out_channels=ch, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=ch, out_channels=out_chans, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor, masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = list()\n",
    "        output = tensor.unsqueeze(1)\n",
    "\n",
    "        # Down-Sampling\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "\n",
    "        # Middle blocks\n",
    "        output = self.mid_conv(output)\n",
    "        for layer in self.middle_layers:\n",
    "            output = output + layer(output)\n",
    "\n",
    "        # Up-Sampling.\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            ds_output = stack.pop()\n",
    "            output = torch.cat([output, ds_output], dim=1) # skip connection - u-net\n",
    "            output = layer(output)\n",
    "\n",
    "        return self.final_layers(output).squeeze(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaA7rmpyj3XF"
   },
   "source": [
    "*(SSIM scores are calculated using the validation part of the dataset)*\n",
    "\n",
    "#### Denser 4-fold\n",
    "\n",
    "<table>\n",
    "       <tr>\n",
    "        <td> epoches | </td>\n",
    "        <td> 3 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> learning rate |</td>\n",
    "        <td>  1e-3 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> weight_decay |</td>\n",
    "        <td> 0 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> Average SSIM score |</td>\n",
    "        <td> 0.6413</td>\n",
    "       </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4m-bT2r4tQA6GTXJlfgLBgFG95Duh1dip7zGUdN7BsRNuX28AWZuEu9glaFiIIWiEiYFZX7gsxf6Xk1UDHjk-gojs7sK9XS33SSRGaczun_wnULHqJpGKQgPWVuNWTueRQAK9UDemIvBYCtyRgzWcM4SQgiNQBo_IKI5dFp_OzfIfYaApkkJ9hb98bOC9GGwDBEJGOPjbEeSiAMV4xGxK_hg/4af-pred-gt.png?psid=1&width=851&height=260)\n",
    "\n",
    "The results could be percived in the figure. On the left, we have the low resolution MRI image, in the middle the prediction of our model and on the right we have the high resolution image taken as reference.\n",
    "\n",
    "We can see also how the loss is decresing in the training.\n",
    "\n",
    "\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4mMeDfCGvjt35lnQq3o9pZrXTYk0lXKazFu4bR_k1qpYe2aFPxJ3sjBz4sabaHxK5l4FLuYceMSLNnnZHkYsdKDQOtOvF-A-_ezEhdHTv_Rx7VdoNPPRWgSK3Tz5W8jg_1Dc0Fvbj7b4RzepS-ftHejkrIcw-TxtNn-meCp5B7a1kKLEmOMpEp9kd2JmjJhV5WeR3KbthJrUCr9NxKBSTJwA/loss.png?psid=1&width=380&height=248)\n",
    "#### Denser early stop 4-fold\n",
    "\n",
    "<table>\n",
    "       <tr>\n",
    "        <td> epoches | </td>\n",
    "        <td> 3 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> learning rate |</td>\n",
    "        <td>  1e-4</td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> weight_decay |</td>\n",
    "        <td>  1e-2</td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> Average SSIM score |</td>\n",
    "        <td>  0.6319</td>\n",
    "       </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4mZT5_xbXgPRv2LYkGAj-uXgOQQheyt_C-JYRpKV-rOAcDo1ZHjphDSwPyR2gwFmcLM2bh9jslkbeJcTBdQZ0sGATTj9b2A2OzCWgNgc2EUkZZpq4h7x5Hxu-Kb7XdqkbWMVqwUAMOJolljbNiw6XNZpXcxfQ1z2Ti0bqzy8rzzRdNW1PThD3ytTfbl5BybPIsP5QcMeQMlq8isvB4YF1pzw/4af-pred-gt.png?psid=1&width=632&height=194)\n",
    "\n",
    "The results could be percived in the figure. On the left, we have the low resolution MRI image, in the middle the prediction of our model and on the right we have the high resolution image taken as reference.\n",
    "\n",
    "We can see also how the loss is changing in the training.\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4miBfx6nCx3tU88s71P-56cNghQ7bcAjogusVagC-5M-OZzbVT7pbTWzIhJpA9TtK01Y_9_MX60cH1Y9713fgoAf0HJjVOkHUS_cGFcV2Wa3PBWeNM-86YpkrCc-WeXERcug9hrynRBjQU9rs8JACsA7iY9CTIhhxRnJehgTWC5YrJmm6bdRDDh9R_x6bqzvfp77L-wcXYTTYpLbJprt7xKQ/loss.png?psid=1&width=380&height=248)\n",
    "#### Denser early stop 8-fold\n",
    "\n",
    "<table>\n",
    "       <tr>\n",
    "        <td> epoches | </td>\n",
    "        <td> 3 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> learning rate |</td>\n",
    "        <td>  1e-3 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> weight_decay |</td>\n",
    "        <td> 0 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> Average SSIM score |</td>\n",
    "        <td> 0.5185</td>\n",
    "       </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4mMpZ2z22GXnSyLTz7o8YqjXqstZpiZwR7WWwR79EyhuLDRIcPZF6idqdjOrgH_P5U29-HU6N5Ns192LBLc0QMCKdM3o9qm1cUywSbDeGCIaepPMGWuhsQ7FBYftKZpIUWZlJSebU3X7_Swu5qUov7knVPpt6teqYhN1NJ4a9bA1o4kj5_4k-4ZmKk-H0-UUM4CSdPbag2bYxJG7ubM8K-Uw/8af-pred-gt.png?psid=1&width=851&height=260)\n",
    "\n",
    "The results could be percived in the figure. On the left, we have the low resolution MRI image, in the middle the prediction of our model and on the right we have the high resolution image taken as reference.\n",
    "\n",
    "We can see also how the loss is decresing gerenally during the training. \n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4mR-H7YXVqGigAEA8LhnTC-DEC2DrCr-9Aw0eQ7OJcWvQRWmZHPupDHUEVxWpHUk6r3L4Qn4DiH7_-WELBWhjSTf8nFYC6eeYWdVrGevbExAfqg2GRVAd9MJBOwIuRiiAH8UM3WpLTpZjZiz-e3xU9OJhnW3CZwk6y5l3aQALcmFZGSJ_E48t7cbICd2QnZlD60V8o0KCNmhLFxROweUY79w/loss.png?psid=1&width=380&height=248)\n",
    "#### Down-Sampling 4-fold\n",
    "Using basic autoencoder model feeding cropped (320x320) absolute valued image through the network.\n",
    "\n",
    "<table>\n",
    "       <tr>\n",
    "        <td> epoches | </td>\n",
    "        <td> 3 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> learning rate |</td>\n",
    "        <td>  1e-3 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> weight_decay |</td>\n",
    "        <td> 0 </td>\n",
    "       </tr>\n",
    "    <tr>\n",
    "        <td> Average SSIM score |</td>\n",
    "        <td> 0.6420</td>\n",
    "       </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4mux54aemO21NTWcmlru_5646mkyIRQq3QUxCQwqmjCUO9Ay05z4YDz0Kxy9FFfQppp21cb1xtNwS9aOgVd4Z34tFQLVYdBIBlvT3pLxHRQNnKkQCjUMX3Uuc27umpr9gnihUkRk0m8lhY1EH3UiXjejqIEbXJ2rSwG9HqSTbo0zb8-DesVnHwRRGBCJDUT-9UZAh5zop1kCkFEBs7FLKKug/loss.png?psid=1&width=380&height=248)\n",
    "\n",
    "The results could be percived in the figure. On the left, we have the low resolution MRI image, in the middle the prediction of our model and on the right we have the high resolution image taken as reference.\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4mh8UgkTSNJMRlp0UCYufJaIa3EWNShFkKHzpZbm682z2iBXajMyUmKb3Cn6Ps6hSWxo-2IUhQo354NfDCNVxgIGrfcmpkbtScdCpEsdZBeIT23gCCiBT9fkgWUZ4of7tQl6XNN57U2m55FDA7iHyoIEuNbUfF6IlrdzVhWihiUK7G86BnxSSOeP9CCTuUEqHxsFzy3ttadFtolGc89w0vHQ/und4af-pred-gt.png?psid=1&width=851&height=260)\n",
    "#### U-net 8-af batch k-space correction\n",
    "We wanted also to apply the k-space correction described in the design part to see if this will improve our results. We implemented it, we display some of the results.\n",
    "\n",
    "![title](https://am3pap001files.storage.live.com/y4mIGbPG3y0SOQ2yIsQ7lM5RWWFORITbwbsNSAVqCIEkLbxzDBEZO569pwWizXsZGqd4McLQRqj79FNfbPPvf_LKuVpKTiYwdqzW1X0ewuiHWJnlHcGZtWTzHoQfs1I96FTVlOZZ3F458Btnq2DT3Vxpv6U9dLqooF7PuoMEueFaLkuRd_GyTMBBo6TSs9ZWKbEy84tGVXXyDBclZQ45rzXww/newresult.png?psid=1&width=859&height=197)\n",
    "\n",
    "On the left we have the undersampled MRI image, the second one is the prediction of the model, the third one is the result after applying k-space correction and the final one on the right the fully sampled MRI image as reference.\n",
    "\n",
    "*(k-space correction wasn't yielding good results because we're working on cropped image which has less information of the fully sampled k-space)*\n",
    "\n",
    "\n",
    "For this U-net 8-af model, it can be clearly presented that the loss of this training tends to zero which has the best performance almost these models tested before.\n",
    "![title](https://am3pap001files.storage.live.com/y4mDodF177XujY1QPe3k_kXKkcn0C4zhpL22FxzBfAVJOSK3rdrEJhjsFHRSEQ_K3Ng33jPzfiSgk1Klydzz_gMTWxqM-HxneEkCLqazegdmfnNh_z_8wZQALGZ9UYvgxB5KMSRK8yndsYWWlkmsLQF3dGy5CD_yWrhyjkKhDjqTKDX5Hpd3KP0Opq3yYlQVvpwKF4yqhBfAPyQ-UyvHvHVBw/loss.png?psid=1&width=378&height=249)\n",
    "\n",
    "### Comparation of these different models\n",
    "\n",
    "\n",
    "|test|name|epoches|learning rate|weight_decay|Average SSIM score|\n",
    "|--|---|--|---|--|---|\n",
    "|1|denser-4af|3|1e-3|0|0.6413|\n",
    "|2|denser-4af|3|1e-4|1e-2|0.6319|\n",
    "|3|denser-8af|3|1e-3|0|0.5185|\n",
    "|4|downsampling-4af|3|1e-3|0|0.6420|\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
